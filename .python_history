_HiStOrY_V2_
from create_index import *
m = EsManagement()
from elasticsearch import Elasticsearch
client = Elasticsearch("http://localhost:9200")
resp = client.info()
resp
client.indices.get_alias("*")
client.search(index="sheets", doc_type="_doc", body = {'size' : 100, 'query': {'match_all' : {}}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'match_all' : {}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'match_all' : {'owner': 'Remi'}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'bool' : {'must': [{'match': {'owner': 'Remi'}}]}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'bool' : {'must': [{'match': {'owner': 'Rei'}}]}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'bool' : {'must': [{'match': {'owner': 'Remi'}}]}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'bool' : {'must': [{'match': {'owner': 'remi'}}]}})
client = Elasticsearch("http://localhost:9200")
client.search(index="sheets", doc_type="_doc", size=1000, query={'bool' : {'must': [{'match': {'owner': 'remi'}}]}})
client.search(index="sheets", doc_type="_doc", size=1000, query={'match_all' : {}})
from elasticsearch import Elasticsearch
client = Elasticsearch("http://localhost:9200")
client.search(index="sheets", doc_type="_doc", size=1000, query={'match_all' : {}})
client = Elasticsearch("http://localhost:9200")
client.search(index="sheets", doc_type="_doc", size=1000, query={'match_all' : {}})
96 % 25
int(96/25) * 25 + 96 % 25
int(96/25) * 25
int(96/20) * 10
int(96/20) * 20 + 10
int(102/20) * 20 + 10
int(10/20) * 20 + 10
int(11/20) * 20 + 10
int(16/20) * 20 + 10
int(20/20) * 20 + 10
import pandas as p
import pandas as pd
pd.Timedelta("1 days")
pd.Timedelta("10 minutes")
pd.Timedelta("10 minutes") + pd.Timestamp('2021-11-15 09:00:00', tz=None)
2*pd.Timedelta("10 minutes") + pd.Timestamp('2021-11-15 09:00:00', tz=None)
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Wikinews
Wikinews.save(dump_path="dumps/enwikinews-latest-pages-meta-current.xml.bz2", max_doc_count=0, folder="enwikinews")
from src import Sources
Sources.save(file="enwikinews/sources.txt", folder="sources", max_workers=5, timeout=10, retry=False)
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from src import Clean
Clean.clean("wikinewssum.json", "filtered_wikinewssum.json")
from elg import Entity
e = Entity.from_id(3967)
e
e.
from elg import Entity
e = Entity.from_id(3967)
e.record
from elg import Entity
entity = Entity.from_id(474)
entity
print(entity)
entity.elg_compatible_service
entity.resource_
entity.resource_type
from elg import Service
entity
from elg import Service
s = Service.from_id(474)
91716f6c-a06a-47c5-b80a-24fccbdd08f1.6326690e-f95a-4ed2-afbf-422dcb28fe46.7f70e03d-f327-4333-8ec9-2b236b432169
from elg import Service
s = Service.from_id(474)
s.elg_execution_location
from elg.local_installation import LocalInstallation
LocalInstallation.generate_docker_compose([474])
from elg.local_installation import LocalInstallation; LocalInstallation.generate_docker_compose([474])
corpus = Corpus.from_id(913)
corpus.download()
from elg import Corpus
corpus = Corpus.from_id(913)
corpus.download()
y
corpus = Corpus.from_id(7498)
corpus.download()
corpus.distributions[0]
dis = corpus.distributions[0]
dis.access_location
dis.is_downloadable()
dis.download_location
dis.licence
dis.licence._get_elg_licence_identifier()
from .utils import (CORPUS_DOWNLOAD_URL, LICENCE_URL, get_argument_from_json,
                    get_domain, get_en_value, get_information,
                    get_metadatarecord, map_metadatarecord_to_result)
from elg.utils import (CORPUS_DOWNLOAD_URL, LICENCE_URL, get_argument_from_json,
                    get_domain, get_en_value, get_information,
                    get_metadatarecord, map_metadatarecord_to_result)
LICENCE_URL.format(domain="live", licence=dis.licence._get_elg_licence_identifier())
LICENCE_URL
dis.licence
dis.licence.__str__()
dis.licence.__repr__()
dis.licence.name
dis.licence.urls
from elg import Corpus
corpus = Corpus.from_id(7498)
dis = corpus.distributions[0]
dis.licence.name
corpus = Corpus.from_id(913)
dis = corpus.distributions[0]
dis.licence.name
dis.licence._get_elg_licence_identifier()
corpus = Corpus.from_id(7498)
dis0 = corpus.distributions[0]
dis0.licence._get_elg_licence_identifier()
from elg import Corpus
corpus = Corpus.from_id(7498)
corpus.download()
y
corpus = Corpus.from_id(913)
corpus.download()
y
from elg import Corpus
corpus = Corpus.from_id(7498)
corpus.download()
from elg import Corpus
corpus = Corpus.from_id(7498)
corpus.download()
from elg import Corpus
corpus = Corpus.from_id(7498)
corpus.download()
y
from elg import Entity
e = Entity.from_id(474)
e.record.service_info
e.record.described_entity.intended_application[0]
e.record.described_entity.lr_subclass.software_distribution[0]
from elg.local_installation import LocalInstallation; LocalInstallation.generate_docker_compose([474])
from elg import Service
s = Service.from_docker_image("techiaith/elg-adapter-deepspeech-server-cy-transcribe:0.1", "http://localhost:8000/process", 8787)
s("/Users/airklizz/Nextcloud/Documents/Travail/ELG/python-services/wav2vec2/audio.wav", "audio", sync_mode=True)
from elg import Service
s = Service.from_docker_image("techiaith/elg-adapter-deepspeech-server-cy-transcribe:0.2", "http://localhost:8000/process", 8787)
s("/Users/airklizz/Nextcloud/Documents/Travail/ELG/python-services/wav2vec2/audio.wav", "audio", sync_mode=True)
dd
from urllib.parse import urlparse
url = urlparse("http://localhost:9898/process/a/s")
url.port
url.path
from elg import Service
s = Service.from_id(474, scope="offline_access")
s.elg_execution_location
from elg.local_installation import LocalInstallation; LocalInstallation.generate_docker_compose([474])
from elg.utils.local_installation import *
from elg.utils.local_installation import .
import elg.utils.local_installation
import elg.utils.local_installation as li
from elg.local_installation import LocalInstallation; LocalInstallation.generate_docker_compose([474])
from elg.local_installation import LocalInstallation; print(LocalInstallation.generate_docker_compose([474]))
from elg.local_installation import LocalInstallation; print(LocalInstallation.generate_docker_compose([474, 613]))
from elg import Entity
e = Entity.from_id(474)
get_information(
                id=id,
                obj=entity.record,
                infos=["described_entity", "lr_subclass", "software_distribution"],
            )
from elg.utils import get_information
get_information(
                id=id,
                obj=entity.record,
                infos=["described_entity", "lr_subclass", "software_distribution"],
            )
entity = e
get_information(
                id=id,
                obj=entity.record,
                infos=["described_entity", "lr_subclass", "software_distribution"],
            )
from elg.local_installation import LocalInstallation; print(LocalInstallation.generate_docker_compose([9226, 9232]))
from elg import Service
s = Service.from_id(9232, local=True)
from elg import Service
s = Service.from_id(9232, local=True)
from elg import Service
s = Service.from_id(9232, local=True)
from elg import Service
s = Service.from_id(9232, local=True)
from elg import Service
s = Service.from_id(9232, local=True)
s.elg_execution_location
s = Service.from_id(9232, local=True, local_domain="https://8080-maroon-junglefowl-7xpf7mz2.ws-eu21.gitpod.io")
s("My name is Rémi")
s = Service.from_id(9232, local=True, local_domain="https://8080-maroon-junglefowl-7xpf7mz2.ws-eu21.gitpod.o")
s("My name is Rémi")
s = Service.from_id(9232, local=True, local_domain="https://8080-maroon-junglefowl-7xpf7mz2.ws-eu21.gitpod.io")
s("My name is Rémi")
s.elg_execution_location
s.elg_execution_location = s.elg_execution_location.replace("/execution", "")
s
s("My name is Rémi")
s.elg_execution_location
s("My name is Rémi")
from elg import Service
s = Service.from_id(9232, local=True, local_domain="https://8080-maroon-junglefowl-7xpf7mz2.ws-eu21.gitpod.io")
s("My name is Rémi")
s.elg_execution_location
from elg.local_installation import LocalInstallation; print(LocalInstallation.generate_docker_compose([9226, 9232, 474]))
from elg import Service
s = Service.from_id(9226, local=True)
from elg.local_installation import LocalInstallation; print(LocalInstallation.generate_docker_compose([9226, 9232, 474]))
from elg import Service
s = Service.from_id(474, scope="offline_access")
s("Test")
s = Service.from_id(474)
8d396d75-5b12-4916-a72a-d05694c8a677.ae69dac5-4179-4d14-a997-1b798472ec79.7f70e03d-f327-4333-8ec9-2b236b432169
s("Test")
from elg import Authentication
Authentication.init(scope="offline_access")
44123dee-99ec-4301-90f6-bacb27d45ddf.9dca7829-90ef-49c4-81c4-26f128f2033a.7f70e03d-f327-4333-8ec9-2b236b432169
au = Authentication.init(scope="offline_access")
d66e56a2-7b2e-4dda-9e2b-f63eb931aafc.9dca7829-90ef-49c4-81c4-26f128f2033a.7f70e03d-f327-4333-8ec9-2b236b432169
au.to_json("~/Desktop")
au.to_json()
au.to_json("")
au.to_json("./")
au.to_json("tokens.json")
mkdir tests
from gensim.test.utils import datapath, get_tmpfile; from gensim.corpora import WikiCorpus, MmCorpus
path_to_wiki_dump("frwikinews-latest-pages-meta-current.xml.bz2")
path_to_wiki_dump = datapath("frwikinews-latest-pages-meta-current.xml.bz2")
corpus_path = get_tmpfile("frwikinews/_bow.mm")
wiki = WikiCorpus(path_to_wiki_dump)
path_to_wiki_dump = datapath("./frwikinews-latest-pages-meta-current.xml.bz2")
wiki = WikiCorpus(path_to_wiki_dump)
path_to_wiki_dump = datapath("/Users/airklizz/Nextcloud/Documents/Travail/ELG/WikinewsSum/tests/frwikinews-latest-pages-meta-current.xml.bz2")
wiki = WikiCorpus(path_to_wiki_dump)
type(wiki)
next(wiki
texts = wiki.get_texts()
type(texts)
next(texts)
for vec in wiki:
	print(vec)
	pass
stream = wiki.getstream()
next(stream)
s = next(stream)
type(s)
s
next(stream)
s
next(stream)
import gensim
pages = gensim.corpora.wikicorpus.extract_pages("/Users/airklizz/Nextcloud/Documents/Travail/ELG/WikinewsSum/tests/frwikinews-latest-pages-meta-current.xml.bz2")
next(pages)
p =next(pages)
p = next(pages)
pages = gensim.corpora.wikicorpus.extract_pages("/Users/airklizz/Nextcloud/Documents/Travail/ELG/WikinewsSum/tests/frwikinews-latest-pages-meta-current.xml.bz2")
type(pages)
for p in pages: print(p)
from gensim import utils
import json
with utils.open("frwikinews-latest.json.gz", "rb") as f:
	data = f
line = next(f)
with utils.open("frwikinews-latest.json.gz", "rb") as f:
	line = next(f)
line
json.loads(line)
with utils.open("frwikinews-latest.json.gz", "rb") as f:
	lines = [];for i in range(1000): lines.append(next(f))
	lines = []
with utils.open("frwikinews-latest.json.gz", "rb") as f:
	lines = []
	for i in range(1000): lines.append(next(f))
len(lines)
lines[-1]
json.loads(lines[-1])
from pprint import pprint
pprint(json.loads(lines[-1]), indent=2)
with utils.open("frwikinews-latest.json.gz", "rb") as f:
	lines = list(f)
len(lines)
from tqdm import tqdm
articles = []
from line in tqdm(lines): articles.append(json.loads(line))
for line in tqdm(lines): articles.append(json.loads(line))
articles[0]
articles[1]
articles[1]["section_texts"][0]
t2t = {a["title"]: a["section_texts"][0] for a in articles}
t2t["Manifestation et concert contre l'« immigration jetable »"]
print(t2t["Manifestation et concert contre l'« immigration jetable »"])
print(t2t['Les parents du "bébé 81" confirmés'])
from datasets import load_dataset
dataset = load_dataset("airKlizz/wikinewssum")
dataset = load_dataset("airKlizz/wikinewssum", use_auth_token=True)
dataset = load_dataset("json", data_files=["https://huggingface.co/datasets/airKlizz/wikinewssum/resolve/main/train.json"], use_auth_token=True)
from datasets import load_dataset
dataset = load_dataset("airKlizz/wikinewssum", use_auth_token=True)
dataset = load_dataset("json", data_files=["https://huggingface.co/datasets/airKlizz/wikinewssum/resolve/main/train.json"], use_auth_token=True)
dataset = load_dataset("airKlizz/wikinewssum", use_auth_token=True)
from datasets import load_dataset
dataset = load_dataset("airKlizz/wikinewssum")
dataset = load_dataset("json", data_files=["https://huggingface.co/datasets/airKlizz/wikinewssum/resolve/main/train.json"])
dataset = load_dataset("json", data_files="https://huggingface.co/datasets/airKlizz/wikinewssum/resolve/main/train.json")
from datasets import load_dataset
dataset = load_dataset("wikinewssum")
dataset = load_dataset("json", data_files={"train": "wikinewssum/train.json", "validation": "wikinewssum/validation.json", "test": "wikinewssum/test.json"})
dataset
train = dataset["train"]
train[0]
train[392]
import nltk
nltk.download('punkt')
from src.oracle import Oracle
from datasets import load_dataset
dataset = load_dataset("json", data_files={"test": "/Users/airklizz/Desktop/test_with_sources_text.json"}, split="test")
oracle = Oracle()
from src.oracle import Oracle
from datasets import load_dataset
dataset = load_dataset("json", data_files={"test": "/Users/airklizz/Desktop/test_with_sources_text.json"}, split="test")
oracle = Oracle()
new_dataset = oracle.add_oracle_sources_text_to_dataset(dataset)
from src.oracle import Oracle
from datasets import load_dataset
dataset = load_dataset("json", data_files={"test": "/Users/airklizz/Desktop/test_with_sources_text.json"}, split="test")
oracle = Oracle()
new_dataset = oracle.add_oracle_sources_text_to_dataset(dataset)
new_dataset
new_dataset.to_json.__docstring__
new_dataset.to_json.__doc___
new_dataset.to_json.__doc__
print(new_dataset.to_json.__doc__)
new_dataset.to_json("test_with_oracle_sources_text.json")
dataset = load_dataset("json", data_files={"validation": "/Users/airklizz/Desktop/validation_with_sources_text.json"}, split="validation")
new_dataset = oracle.add_oracle_sources_text_to_dataset(dataset)
new_dataset.to_json("validation_with_oracle_sources_text.json")
